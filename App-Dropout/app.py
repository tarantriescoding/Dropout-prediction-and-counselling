# app.py
# UmeedRise ‚Äì Simple, Trustworthy Student Dropout Prediction & Support
# Designed for schools, teachers, students, and communities with clear language and guided steps.
# Author: Copilot

import os
import io
import json
import warnings
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
)
from xgboost import XGBClassifier
import shap
import plotly.graph_objs as go
import plotly.express as px
import joblib
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")

# -----------------------------
# Page config & accessible UI
# -----------------------------
st.set_page_config(
    page_title="UmeedRise ‚Äì Simple Student Support",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Light gradient & accessible visual style
st.markdown(
    """
    <style>
    .stApp {
        background: linear-gradient(135deg, #E8ECFF 0%, #C7C3F5 100%);
    }
    .glass-card {
        background: rgba(255, 255, 255, 0.65);
        border-radius: 18px;
        box-shadow: 0 8px 28px rgba(31, 38, 135, 0.20);
        backdrop-filter: blur(7px);
        border: 1px solid rgba(255, 255, 255, 0.25);
        padding: 18px;
        margin-bottom: 18px;
    }
    h1, h2, h3 {
        color: #20204A;
        font-weight: 700;
        letter-spacing: 0.2px;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
    }
    p, span, label, div, li {
        color: #1F2756;
        font-size: 16px;
        line-height: 1.6;
        font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
    }
    [data-testid="stSidebar"] {
        background: rgba(255, 255, 255, 0.65);
        backdrop-filter: blur(6px);
        border-right: 1px solid rgba(255,255,255,0.3);
    }
    .stButton>button, .stDownloadButton>button {
        border-radius: 12px;
        font-weight: 600;
        padding: 10px 16px;
        border: none;
    }
    .stButton>button {
        background-color: #5B6EF0;
        color: white;
    }
    .stDownloadButton>button {
        background-color: #4FB37A;
        color: white;
    }
    .alert-banner {
        border-left: 6px solid #D8345F;
        background: rgba(216, 52, 95, 0.14);
        padding: 12px;
        border-radius: 10px;
        margin-bottom: 12px;
    }
    .medium-banner {
        border-left: 6px solid #F2A007;
        background: rgba(242, 160, 7, 0.15);
        padding: 12px;
        border-radius: 10px;
        margin-bottom: 12px;
    }
    .success-banner {
        border-left: 6px solid #4FB37A;
        background: rgba(79, 179, 122, 0.15);
        padding: 12px;
        border-radius: 10px;
        margin-bottom: 12px;
    }
    .badge {
        display: inline-block;
        padding: 6px 10px;
        border-radius: 12px;
        font-weight: 700;
        font-size: 0.95rem;
    }
    .badge-low { background: #DDF7E7; color: #0F6A44; }
    .badge-medium { background: #FFF2CC; color: #6C4A00; }
    .badge-high { background: #FFE3E8; color: #9B1028; }
    .stDataFrame, .stTable { border-radius: 10px; overflow: hidden; }
    .helper {
        font-size: 14px;
        color: #3A3A70;
        background: rgba(255,255,255,0.55);
        border: 1px dashed rgba(0,0,0,0.15);
        padding: 10px;
        border-radius: 10px;
        margin-top: 8px;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# -----------------------------
# Language support (English/Hindi)
# -----------------------------
LANG = st.sidebar.selectbox("Language / ‡§≠‡§æ‡§∑‡§æ", ["English", "‡§π‡§ø‡§Ç‡§¶‡•Ä"])

def T(en: str, hi: str) -> str:
    return hi if LANG == "‡§π‡§ø‡§Ç‡§¶‡•Ä" else en

# -----------------------------
# Sidebar navigation & simple mode
# -----------------------------
st.sidebar.title("üéì UmeedRise")
st.sidebar.markdown(T("Simple student support for schools & communities.", "‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§î‡§∞ ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§∞‡§≤ ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ‡•§"))

mode = st.sidebar.radio(T("Mode", "‡§Æ‡•ã‡§°"), options=[T("Simple", "‡§∏‡§∞‡§≤"), T("Advanced", "‡§â‡§®‡•ç‡§®‡§§")], index=0)

page = st.sidebar.radio(
    T("Navigate", "‡§®‡•á‡§µ‡§ø‡§ó‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç"),
    options=[
        T("Home", "‡§π‡•ã‡§Æ"),
        T("Upload & Train", "‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç"),
        T("Dashboard", "‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§°"),
        T("Student Search", "‡§õ‡§æ‡§§‡•ç‡§∞ ‡§ñ‡•ã‡§ú"),
        T("Explainability (SHAP)", "‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡•á‡§Ø‡§§‡§æ (SHAP)"),
        T("Counseling & Mentoring", "‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§î‡§∞ ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"),
    ],
    index=0,
)

# Risk thresholds & hyperparameters (simple defaults vs advanced controls)
st.sidebar.markdown("---")
st.sidebar.subheader(T("Settings", "‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏"))
if mode == T("Simple", "‡§∏‡§∞‡§≤"):
    low_th = 0.40
    high_th = 0.70
    alert_high = 0.80
    params = {"n_estimators": 300, "max_depth": 5, "learning_rate": 0.08, "scale_pos_weight": 1.0}
    st.sidebar.markdown(T("Using safe defaults for accuracy and stability.", "‡§∂‡•Å‡§¶‡•ç‡§ß‡§§‡§æ ‡§î‡§∞ ‡§∏‡•ç‡§•‡§ø‡§∞‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡•§"))
else:
    low_th = st.sidebar.slider(T("Low risk threshold", "‡§ï‡§Æ ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡•Ä‡§Æ‡§æ"), 0.1, 0.6, 0.4, 0.05)
    high_th = st.sidebar.slider(T("High risk threshold", "‡§â‡§ö‡•ç‡§ö ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡•Ä‡§Æ‡§æ"), 0.5, 0.95, 0.7, 0.05)
    alert_high = st.sidebar.slider(T("Alert high-risk cutoff", "‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§â‡§ö‡•ç‡§ö ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§ï‡§ü‡§ë‡§´"), 0.7, 0.95, 0.80, 0.01)
    st.sidebar.markdown(T("XGBoost Hyperparameters", "XGBoost ‡§π‡§æ‡§á‡§™‡§∞‡§™‡•à‡§∞‡§æ‡§Æ‡•Ä‡§ü‡§∞"))
    n_estimators = st.sidebar.number_input("n_estimators", 50, 1000, 300, 50)
    max_depth = st.sidebar.number_input("max_depth", 2, 12, 5, 1)
    learning_rate = st.sidebar.slider("learning_rate", 0.01, 0.3, 0.08, 0.01)
    scale_pos_weight = st.sidebar.slider(T("scale_pos_weight (class imbalance)", "scale_pos_weight (‡§ï‡§ï‡•ç‡§∑‡§æ ‡§Ö‡§∏‡§Ç‡§§‡•Å‡§≤‡§®)"), 0.5, 10.0, 1.0, 0.5)
    params = {
        "n_estimators": int(n_estimators),
        "max_depth": int(max_depth),
        "learning_rate": float(learning_rate),
        "scale_pos_weight": float(scale_pos_weight),
    }

# -----------------------------
# State
# -----------------------------
for key in ["df", "detected", "train_result", "pred_df", "risk_labels", "alerts_df"]:
    if key not in st.session_state:
        st.session_state[key] = None

MODEL_PATH = "umeedrise_model.joblib"
PREPROCESSOR_PATH = "umeedrise_preprocessor.joblib"
METADATA_PATH = "umeedrise_metadata.json"

# -----------------------------
# Helpers
# -----------------------------
def safe_float(x):
    try:
        return float(x)
    except Exception:
        return np.nan

def _normalized(col: str) -> str:
    return col.strip().lower().replace(" ", "").replace("_", "")

def load_data(uploaded_file) -> pd.DataFrame:
    """
    Robust CSV loader with safe fallbacks.
    """
    try:
        df = pd.read_csv(uploaded_file, on_bad_lines="skip")
    except UnicodeDecodeError:
        df = pd.read_csv(uploaded_file, encoding="latin1", on_bad_lines="skip")
    except Exception:
        data = uploaded_file.read()
        df = pd.read_csv(io.BytesIO(data), on_bad_lines="skip")
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    return df

def detect_columns(df: pd.DataFrame) -> Dict[str, any]:
    """
    Auto-detect target, ID, numeric, categorical, attendance, and marks columns.
    """
    cols = df.columns.tolist()
    normalized_map = {c: _normalized(c) for c in cols}

    target_candidates = ["dropout", "label", "target", "isdropout", "risk", "y"]
    detected_target = None
    for c, norm in normalized_map.items():
        if any(norm == _normalized(tc) for tc in target_candidates):
            detected_target = c
            break
    if detected_target is None:
        for c in cols:
            unique = df[c].dropna().unique()
            if len(unique) <= 3:
                vals = set([str(v).strip().lower() for v in unique])
                binary_like = vals.issubset({"0", "1", "true", "false", "yes", "no"})
                if binary_like:
                    detected_target = c
                    break

    id_candidates = ["studentid","id","roll","rollno","rollnumber","email","admissionno","admno","regno","registrationno","name","fullname"]
    detected_id = None
    for c, norm in normalized_map.items():
        if any(norm == _normalized(ic) for ic in id_candidates):
            detected_id = c
            break
    if detected_id is None:
        for c in cols:
            if df[c].dtype == "object" and df[c].nunique() > max(10, int(0.2 * len(df))):
                detected_id = c
                break

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = [c for c in cols if c not in numeric_cols]

    if detected_target in numeric_cols:
        numeric_cols = [c for c in numeric_cols if c != detected_target]
    if detected_target in categorical_cols:
        categorical_cols = [c for c in categorical_cols if c != detected_target]

    attendance_cols = [c for c in cols if "attendance" in _normalized(c)]
    marks_cols = [c for c in cols if any(k in _normalized(c) for k in ["marks","score","grade","percentage"])]

    return {
        "target": detected_target,
        "id_col": detected_id,
        "numeric_cols": numeric_cols,
        "categorical_cols": categorical_cols,
        "attendance_cols": attendance_cols,
        "marks_cols": marks_cols,
    }

def build_preprocessor(numeric_cols: List[str], categorical_cols: List[str]) -> ColumnTransformer:
    """
    Numeric: Median Imputer + StandardScaler
    Categorical: Most-frequent Imputer + OneHotEncoder(sparse_output=False)
    """
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
    ])
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ]
    )
    return preprocessor

def _make_binary_target(y: pd.Series) -> np.ndarray:
    """
    Convert target to 0/1 consistently from various formats.
    """
    y_clean = y.copy()
    if y_clean.dtype == "object":
        y_lower = y_clean.astype(str).str.strip().str.lower()
        y_bin = y_lower.map(
            lambda v: 1 if v in ["1","true","yes","y","dropout","high","risk"] else 0
        )
    else:
        y_bin = y_clean.map(lambda v: 1 if str(v).strip().lower() in ["1","true"] else 0)
    y_bin = y_bin.fillna(0).astype(int).values
    return y_bin

def _auto_scale_pos_weight(y: np.ndarray) -> float:
    """
    Auto compute scale_pos_weight for class imbalance: (negatives / positives).
    """
    pos = np.sum(y == 1)
    neg = np.sum(y == 0)
    if pos == 0:
        return 1.0
    return max(1.0, float(neg) / float(pos))

def train_xgboost(
    df: pd.DataFrame,
    target_col: str,
    id_col: Optional[str],
    numeric_cols: List[str],
    categorical_cols: List[str],
    params: Dict[str, any],
    test_size: float = 0.2,
    random_state: int = 42
) -> Dict[str, any]:
    """
    Train XGBClassifier with robust preprocessing and artifact saving.
    """
    X = df[numeric_cols + categorical_cols].copy()
    y = _make_binary_target(df[target_col])

    # Auto-adjust scale_pos_weight if simple mode
    if mode == T("Simple", "‡§∏‡§∞‡§≤"):
        params["scale_pos_weight"] = _auto_scale_pos_weight(y)

    stratify = y if len(np.unique(y)) > 1 else None
    preprocessor = build_preprocessor(numeric_cols, categorical_cols)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=stratify, random_state=random_state
    )

    model = XGBClassifier(
        n_estimators=int(params.get("n_estimators", 300)),
        max_depth=int(params.get("max_depth", 5)),
        learning_rate=float(params.get("learning_rate", 0.08)),
        scale_pos_weight=float(params.get("scale_pos_weight", 1.0)),
        subsample=0.9,
        colsample_bytree=0.9,
        random_state=random_state,
        n_jobs=-1,
        eval_metric="logloss",
        tree_method="auto",
        use_label_encoder=False
    )

    X_train_proc = preprocessor.fit_transform(X_train)
    X_test_proc = preprocessor.transform(X_test)
    model.fit(X_train_proc, y_train)

    # Feature names
    try:
        num_features = numeric_cols
        ohe: OneHotEncoder = preprocessor.named_transformers_["cat"].named_steps["onehot"]
        ohe_features = ohe.get_feature_names_out(categorical_cols).tolist()
        feature_names = num_features + ohe_features
    except Exception:
        feature_names = [f"f_{i}" for i in range(X_train_proc.shape[1])]

    # Save artifacts
    try:
        joblib.dump(model, MODEL_PATH)
        joblib.dump(preprocessor, PREPROCESSOR_PATH)
        meta = {
            "target_col": target_col,
            "id_col": id_col,
            "numeric_cols": numeric_cols,
            "categorical_cols": categorical_cols,
            "feature_names": feature_names,
        }
        with open(METADATA_PATH, "w") as f:
            json.dump(meta, f)
    except Exception:
        pass

    return {
        "preprocessor": preprocessor,
        "model": model,
        "X_train": X_train,
        "X_train_proc": X_train_proc,
        "X_test": X_test,
        "X_test_proc": X_test_proc,
        "y_train": y_train,
        "y_test": y_test,
        "feature_names": feature_names,
    }

def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:
    metrics = {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall": recall_score(y_true, y_pred, zero_division=0),
        "f1": f1_score(y_true, y_pred, zero_division=0),
    }
    try:
        metrics["roc_auc"] = roc_auc_score(y_true, y_prob)
    except Exception:
        metrics["roc_auc"] = np.nan
    return metrics

def predict_student(
    model: XGBClassifier,
    preprocessor: ColumnTransformer,
    df: pd.DataFrame,
    feature_cols: List[str],
) -> pd.DataFrame:
    X = df[feature_cols].copy()
    X_proc = preprocessor.transform(X)
    prob = model.predict_proba(X_proc)[:, 1]
    pred = (prob >= 0.5).astype(int)
    return pd.DataFrame({"probability": prob, "prediction": pred})

def risk_label(prob: float, thresholds: Tuple[float, float] = (0.4, 0.7)) -> str:
    low, high = thresholds
    if prob < low:
        return "Low"
    elif prob < high:
        return "Medium"
    else:
        return "High"

def explain_shap(
    model: XGBClassifier,
    X_proc: np.ndarray,
    feature_names: List[str],
    index: Optional[int] = None,
) -> Dict[str, any]:
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_proc)
    mean_abs = np.mean(np.abs(shap_values), axis=0)
    importance_df = pd.DataFrame({"feature": feature_names, "importance": mean_abs}).sort_values("importance", ascending=False)
    sample_idx = index if index is not None else 0
    sample_shap = shap_values[sample_idx]
    base_value = explainer.expected_value
    return {"shap_values": shap_values, "importance_df": importance_df, "sample_shap": sample_shap, "base_value": base_value}

def generate_counseling(risk: str) -> List[str]:
    if risk == "Low":
        return [T("Continue consistent study habits", "‡§≤‡§ó‡§æ‡§§‡§æ‡§∞ ‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§® ‡§Ü‡§¶‡§§‡•á‡§Ç ‡§ú‡§æ‡§∞‡•Ä ‡§∞‡§ñ‡•á‡§Ç"),
                T("Maintain attendance", "‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡•á‡§Ç")]
    elif risk == "Medium":
        return [T("Teacher mentoring", "‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"),
                T("Assignment discipline", "‡§Ö‡§∏‡§æ‡§á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§Ö‡§®‡•Å‡§∂‡§æ‡§∏‡§®"),
                T("Light peer mentoring", "‡§π‡§≤‡•ç‡§ï‡§æ ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®")]
    else:
        return [T("Immediate counselor meeting", "‡§ï‡§æ‡§â‡§Ç‡§∏‡§≤‡§∞ ‡§∏‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§Æ‡§ø‡§≤‡•á‡§Ç"),
                T("Personalized academic plan", "‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§∂‡•à‡§ï‡•ç‡§∑‡§£‡§ø‡§ï ‡§Ø‡•ã‡§ú‡§®‡§æ"),
                T("Emotional support", "‡§≠‡§æ‡§µ‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∏‡§π‡§Ø‡•ã‡§ó"),
                T("Weekly follow-up", "‡§∏‡§æ‡§™‡•ç‡§§‡§æ‡§π‡§ø‡§ï ‡§´‡•â‡§≤‡•ã-‡§Ö‡§™"),
                T("Mandatory peer mentoring assignment", "‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§® ‡§Ö‡§∏‡§æ‡§á‡§®‡§Æ‡•á‡§Ç‡§ü")]

def _attendance_summary(df: pd.DataFrame, attendance_cols: List[str]) -> Optional[str]:
    if not attendance_cols:
        return None
    try:
        vals = df[attendance_cols].applymap(safe_float).mean(axis=1)
        return T(f"Avg attendance ~ {np.nanmean(vals):.1f}", f"‡§î‡§∏‡§§ ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ~ {np.nanmean(vals):.1f}")
    except Exception:
        return None

def _marks_summary(df: pd.DataFrame, marks_cols: List[str]) -> Optional[str]:
    if not marks_cols:
        return None
    try:
        vals = df[marks_cols].applymap(safe_float).mean(axis=1)
        return T(f"Avg marks ~ {np.nanmean(vals):.1f}", f"‡§î‡§∏‡§§ ‡§Ö‡§Ç‡§ï ~ {np.nanmean(vals):.1f}")
    except Exception:
        return None

def make_confusion_matrix_plot(cm: np.ndarray) -> go.Figure:
    fig = go.Figure(data=go.Heatmap(
        z=cm,
        x=["Pred 0", "Pred 1"],
        y=["True 0", "True 1"],
        colorscale="Blues",
        showscale=True
    ))
    fig.update_layout(title=T("Confusion Matrix","‡§ï‡§®‡•ç‡§´‡•ç‡§Ø‡•Ç‡§ú‡§º‡§® ‡§Æ‡•à‡§ü‡•ç‡§∞‡§ø‡§ï‡•ç‡§∏"),
                      xaxis_title=T("Predicted","‡§Ö‡§®‡•Å‡§Æ‡§æ‡§®‡§ø‡§§"),
                      yaxis_title=T("Actual","‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï"),
                      margin=dict(l=0, r=0, t=30, b=0), height=350)
    return fig

def make_risk_distribution_plot(probs: np.ndarray) -> go.Figure:
    fig = px.histogram(x=probs, nbins=30, color_discrete_sequence=["#5B6EF0"])
    fig.update_layout(title=T("Risk probability distribution","‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ ‡§µ‡§ø‡§§‡§∞‡§£"),
                      xaxis_title=T("Dropout probability","‡§°‡•ç‡§∞‡•â‡§™‡§Ü‡§â‡§ü ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ"),
                      yaxis_title=T("Count","‡§ó‡§ø‡§®‡§§‡•Ä"),
                      margin=dict(l=0, r=0, t=30, b=0), height=350)
    return fig

def make_risk_pie(labels: List[str]) -> go.Figure:
    fig = px.pie(names=labels, title=T("Risk level composition","‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ"), color=labels,
                 color_discrete_map={"Low":"#4FB37A","Medium":"#F2A007","High":"#D8345F"})
    fig.update_layout(height=350)
    return fig

def make_importance_bar(df_imp: pd.DataFrame, top_n: int = 20) -> go.Figure:
    data = df_imp.head(top_n).iloc[::-1]
    fig = go.Figure(go.Bar(x=data["importance"], y=data["feature"], orientation="h", marker_color="#5B6EF0"))
    fig.update_layout(title=T(f"Top {top_n} features by SHAP importance", f"SHAP ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§∂‡•Ä‡§∞‡•ç‡§∑ {top_n} ‡§´‡•Ä‡§ö‡§∞‡•ç‡§∏"),
                      xaxis_title=T("Mean |SHAP|","‡§î‡§∏‡§§ |SHAP|"),
                      yaxis_title=T("Feature","‡§´‡•Ä‡§ö‡§∞"),
                      margin=dict(l=0, r=0, t=30, b=0), height=500)
    return fig

def build_alerts(pred_df: pd.DataFrame, thresholds: Tuple[float, float], alert_high: float, df: pd.DataFrame, attendance_cols: List[str]) -> pd.DataFrame:
    attend_flag = None
    if attendance_cols:
        try:
            att_val = df[attendance_cols].applymap(safe_float).mean(axis=1)
            attend_flag = att_val < np.nanpercentile(att_val, 20)
        except Exception:
            attend_flag = None

    alerts = []
    for i, r in pred_df.iterrows():
        if r["probability"] >= alert_high:
            alerts.append((T(f"High Risk ‚â• {alert_high:.2f}", f"‡§â‡§ö‡•ç‡§ö ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‚â• {alert_high:.2f}"), i, "High"))
        if thresholds[0] <= r["probability"] < thresholds[1]:
            alerts.append((T("Repeated Medium Risk (pattern)", "‡§¨‡§æ‡§∞-‡§¨‡§æ‡§∞ ‡§Æ‡§ß‡•ç‡§Ø‡§Æ ‡§ú‡•ã‡§ñ‡§ø‡§Æ (‡§™‡•à‡§ü‡§∞‡•ç‡§®)"), i, "Medium"))
        if attend_flag is not None and attend_flag.iloc[i]:
            alerts.append((T("Low Attendance", "‡§ï‡§Æ ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø"), i, "Medium"))
    return pd.DataFrame(alerts, columns=["Alert", "Index", "Severity"])

def style_risk_cell(label: str) -> str:
    if label == "Low":
        text = T("Low","‡§ï‡§Æ")
        return f'<span class="badge badge-low">{text}</span>'
    elif label == "Medium":
        text = T("Medium","‡§Æ‡§ß‡•ç‡§Ø‡§Æ")
        return f'<span class="badge badge-medium">{text}</span>'
    else:
        text = T("High","‡§â‡§ö‡•ç‡§ö")
        return f'<span class="badge badge-high">{text}</span>'

# -----------------------------
# Home
# -----------------------------
if page == T("Home","‡§π‡•ã‡§Æ"):
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.title("UmeedRise")
    st.subheader(T("Simple, clear student dropout prediction & support", "‡§∏‡§∞‡§≤ ‡§î‡§∞ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§°‡•ç‡§∞‡•â‡§™‡§Ü‡§â‡§ü ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø‡§µ‡§æ‡§£‡•Ä ‡§î‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ"))
    st.write(T(
        "Follow three steps: 1) Upload data 2) Train model 3) See dashboard & student support.",
        "‡§§‡•Ä‡§® ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡•á‡§Ç: 1) ‡§°‡•á‡§ü‡§æ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç 2) ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç 3) ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§° ‡§î‡§∞ ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§"
    ))
    st.markdown('</div>', unsafe_allow_html=True)

    with st.container():
        c1, c2, c3 = st.columns(3)
        with c1:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.markdown("### üîÑ " + T("Auto-detection","‡§ë‡§ü‡•ã-‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§®"))
            st.write(T("Target, ID, numeric, categorical columns detected safely.", "‡§ü‡§æ‡§∞‡•ç‡§ó‡•á‡§ü, ‡§Ü‡§à‡§°‡•Ä, ‡§®‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§∞‡§ø‡§ï ‡§î‡§∞ ‡§ï‡•à‡§ü‡•á‡§ó‡•ã‡§∞‡§ø‡§ï‡§≤ ‡§ï‡•â‡§≤‡§Æ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡§π‡§ö‡§æ‡§®‡•á‡§Ç‡•§"))
            st.markdown('</div>', unsafe_allow_html=True)
        with c2:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.markdown("### üß† " + T("Explainable AI","‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡•á‡§Ø ‡§è‡§Ü‡§à"))
            st.write(T("See which features affect risk (SHAP).", "‡§¶‡•á‡§ñ‡•á‡§Ç ‡§ï‡•å‡§® ‡§∏‡•á ‡§´‡•Ä‡§ö‡§∞ ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç (SHAP)‡•§"))
            st.markdown('</div>', unsafe_allow_html=True)
        with c3:
            st.markdown('<div class="glass-card">', unsafe_allow_html=True)
            st.markdown("### ü§ù " + T("Alerts & Mentoring","‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§î‡§∞ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"))
            st.write(T("Get alerts and easy counseling plans; auto peer mentoring.", "‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§î‡§∞ ‡§∏‡§∞‡§≤ ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§Ø‡•ã‡§ú‡§®‡§æ‡§è‡§Å; ‡§ë‡§ü‡•ã ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®‡•§"))
            st.markdown('</div>', unsafe_allow_html=True)

    st.markdown('<div class="glass-card helper">', unsafe_allow_html=True)
    st.write(T("Tip: Start in Simple mode for one-click training.", "‡§ü‡§ø‡§™: ‡§è‡§ï-‡§ï‡•ç‡§≤‡§ø‡§ï ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§∞‡§≤ ‡§Æ‡•ã‡§° ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç‡•§"))
    st.markdown('</div>', unsafe_allow_html=True)

# -----------------------------
# Upload & Train
# -----------------------------
elif page == T("Upload & Train","‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç"):
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.markdown("### üìÅ " + T("Upload dataset","‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç"))
    uploaded_file = st.file_uploader(T("Upload CSV file","CSV ‡§´‡§º‡§æ‡§á‡§≤ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç"), type=["csv"])
    st.markdown('</div>', unsafe_allow_html=True)

    if uploaded_file is not None:
        df = load_data(uploaded_file)
        st.session_state.df = df

        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("### üß≠ " + T("Auto-detection results","‡§ë‡§ü‡•ã-‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ"))
        detected = detect_columns(df)
        st.session_state.detected = detected

        colA, colB, colC = st.columns(3)
        with colA:
            st.write(f"**{T('Label:','‡§≤‡•á‡§¨‡§≤:')}** " + T("Target column","‡§ü‡§æ‡§∞‡•ç‡§ó‡•á‡§ü ‡§ï‡•â‡§≤‡§Æ"))
            st.info(detected["target"] if detected["target"] else T("Not found ‚Äì please select below","‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ ‚Äì ‡§®‡•Ä‡§ö‡•á ‡§ö‡§Ø‡§® ‡§ï‡§∞‡•á‡§Ç"))
        with colB:
            st.write(f"**{T('Label:','‡§≤‡•á‡§¨‡§≤:')}** " + T("ID column","‡§Ü‡§à‡§°‡•Ä ‡§ï‡•â‡§≤‡§Æ"))
            st.info(detected["id_col"] if detected["id_col"] else T("Not found ‚Äì optional","‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ ‚Äì ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï"))
        with colC:
            st.write(f"**{T('Label:','‡§≤‡•á‡§¨‡§≤:')}** " + T("Feature counts","‡§´‡•Ä‡§ö‡§∞ ‡§ó‡§£‡§®‡§æ"))
            st.info(T(f"Numeric: {len(detected['numeric_cols'])}, Categorical: {len(detected['categorical_cols'])}",
                      f"‡§®‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§∞‡§ø‡§ï: {len(detected['numeric_cols'])}, ‡§ï‡•à‡§ü‡•á‡§ó‡•ã‡§∞‡§ø‡§ï‡§≤: {len(detected['categorical_cols'])}"))
        st.markdown('</div>', unsafe_allow_html=True)

        # Manual overrides (kept simple)
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("### üõ†Ô∏è " + T("Confirm or adjust columns","‡§ï‡•â‡§≤‡§Æ ‡§ï‡•Ä ‡§™‡•Å‡§∑‡•ç‡§ü‡§ø/‡§∏‡§Æ‡§æ‡§Ø‡•ã‡§ú‡§® ‡§ï‡§∞‡•á‡§Ç"))
        target_col = st.selectbox(T("Select target column (0/1, Yes/No preferred)","‡§ü‡§æ‡§∞‡•ç‡§ó‡•á‡§ü ‡§ï‡•â‡§≤‡§Æ ‡§ö‡•Å‡§®‡•á‡§Ç (0/1, ‡§π‡§æ‡§Å/‡§®‡§π‡•Ä‡§Ç ‡§¨‡•á‡§π‡§§‡§∞)"),
                                  options=df.columns.tolist(),
                                  index=(df.columns.tolist().index(detected["target"]) if detected["target"] in df.columns else 0))
        id_col = st.selectbox(T("Select ID column (optional)","‡§Ü‡§à‡§°‡•Ä ‡§ï‡•â‡§≤‡§Æ ‡§ö‡•Å‡§®‡•á‡§Ç (‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï)"),
                              options=["None"] + df.columns.tolist(),
                              index=(df.columns.tolist().index(detected["id_col"]) + 1 if detected["id_col"] in df.columns else 0))

        all_feature_cols = [c for c in df.columns if c != target_col]
        default_num = [c for c in detected["numeric_cols"] if c in all_feature_cols]
        default_cat = [c for c in detected["categorical_cols"] if c in all_feature_cols and c != target_col]

        if mode == T("Simple","‡§∏‡§∞‡§≤"):
            # Hide complexity: use detected automatically
            numeric_cols = default_num
            categorical_cols = default_cat
            st.write(T("Using auto-selected features.","‡§ë‡§ü‡•ã-‡§ö‡§Ø‡§®‡§ø‡§§ ‡§´‡•Ä‡§ö‡§∞‡•ç‡§∏ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡•§"))
        else:
            numeric_cols = st.multiselect(T("Numeric feature columns","‡§®‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§∞‡§ø‡§ï ‡§´‡•Ä‡§ö‡§∞ ‡§ï‡•â‡§≤‡§Æ"), options=all_feature_cols, default=default_num)
            categorical_cols = st.multiselect(T("Categorical feature columns","‡§ï‡•à‡§ü‡•á‡§ó‡•ã‡§∞‡§ø‡§ï‡§≤ ‡§´‡•Ä‡§ö‡§∞ ‡§ï‡•â‡§≤‡§Æ"),
                                              options=[c for c in all_feature_cols if c not in numeric_cols], default=default_cat)
        st.markdown('</div>', unsafe_allow_html=True)

        # Preview & stats (simple)
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("### üëÄ " + T("Preview & stats","‡§™‡•ç‡§∞‡•Ä‡§µ‡•ç‡§Ø‡•Ç ‡§î‡§∞ ‡§Ü‡§Å‡§ï‡§°‡§º‡•á"))
        st.dataframe(df.head(15), use_container_width=True)
        st.write(T(f"Rows: {len(df)}, Columns: {len(df.columns)}", f"‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡§Ø‡§æ‡§Å: {len(df)}, ‡§ï‡•â‡§≤‡§Æ: {len(df.columns)}"))
        st.write("**" + T("Missing values per column","‡§™‡•ç‡§∞‡§§‡§ø ‡§ï‡•â‡§≤‡§Æ ‡§Æ‡§ø‡§∏‡§ø‡§Ç‡§ó ‡§µ‡•à‡§≤‡•ç‡§Ø‡•Ç") + "**")
        st.dataframe(df.isna().sum().to_frame(T("missing","‡§Æ‡§ø‡§∏‡§ø‡§Ç‡§ó")).sort_values(T("missing","‡§Æ‡§ø‡§∏‡§ø‡§Ç‡§ó"), ascending=False), use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)

        # Train button
        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
        st.markdown("### üöÄ " + T("Train model","‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç"))
        label_train = T("One-click Train","‡§è‡§ï-‡§ï‡•ç‡§≤‡§ø‡§ï ‡§ü‡•ç‡§∞‡•á‡§®")
        if st.button(label_train):
            if len(numeric_cols) + len(categorical_cols) == 0:
                st.error(T("Please select at least one feature column.","‡§ï‡•É‡§™‡§Ø‡§æ ‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ ‡§è‡§ï ‡§´‡•Ä‡§ö‡§∞ ‡§ï‡•â‡§≤‡§Æ ‡§ö‡•Å‡§®‡•á‡§Ç‡•§"))
            else:
                try:
                    id_selected = None if id_col == "None" else id_col
                    train_res = train_xgboost(df, target_col, id_selected, numeric_cols, categorical_cols, params)
                    st.session_state.train_result = train_res

                    # Evaluate
                    y_test = train_res["y_test"]
                    X_test_proc = train_res["X_test_proc"]
                    model = train_res["model"]
                    y_prob = model.predict_proba(X_test_proc)[:, 1]
                    y_pred = (y_prob >= 0.5).astype(int)
                    metrics = evaluate_model(y_test, y_pred, y_prob)

                    st.success(T("Model trained successfully.","‡§Æ‡•â‡§°‡§≤ ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§ü‡•ç‡§∞‡•á‡§® ‡§π‡•Å‡§Ü‡•§"))
                    st.write(f"**{T('Accuracy','‡§∏‡§ü‡•Ä‡§ï‡§§‡§æ')}:** {metrics['accuracy']:.3f}")
                    st.write(f"**{T('Precision','‡§™‡•ç‡§∞‡§ø‡§∏‡•Ä‡§ú‡§®')}:** {metrics['precision']:.3f}")
                    st.write(f"**{T('Recall','‡§∞‡§ø‡§ï‡•â‡§≤')}:** {metrics['recall']:.3f}")
                    st.write(f"**{T('F1-score','F1-‡§∏‡•ç‡§ï‡•ã‡§∞')}:** {metrics['f1']:.3f}")
                    st.write(f"**{T('ROC-AUC','ROC-AUC')}:** {metrics['roc_auc']:.3f}")

                    # Confusion matrix
                    cm = confusion_matrix(y_test, y_pred)
                    st.plotly_chart(make_confusion_matrix_plot(cm), use_container_width=True)

                    # Full predictions
                    feature_cols = numeric_cols + categorical_cols
                    full_pred = predict_student(model, train_res["preprocessor"], df, feature_cols)
                    risk_labels = [risk_label(p, thresholds=(low_th, high_th)) for p in full_pred["probability"]]
                    st.session_state.pred_df = full_pred
                    st.session_state.risk_labels = risk_labels

                    # Alerts
                    alerts_df = build_alerts(full_pred, (low_th, high_th), alert_high, df, detected["attendance_cols"])
                    st.session_state.alerts_df = alerts_df

                    merged = df.copy()
                    merged["dropout_probability"] = full_pred["probability"]
                    merged["risk_level"] = risk_labels

                    st.download_button(T("Download predictions CSV","‡§™‡•ç‡§∞‡•á‡§°‡§ø‡§ï‡•ç‡§∂‡§®‡•ç‡§∏ CSV ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç"),
                                       data=merged.to_csv(index=False).encode("utf-8"),
                                       file_name="umeedrise_predictions.csv", mime="text/csv")
                except Exception as e:
                    st.error(T(f"Training failed: {e}", f"‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§µ‡§ø‡§´‡§≤: {e}"))
        st.markdown('</div>', unsafe_allow_html=True)
    else:
        st.info(T("Upload a CSV to begin.","‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è CSV ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç‡•§"))

# -----------------------------
# Dashboard
# -----------------------------
elif page == T("Dashboard","‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§°"):
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.markdown("### üìä " + T("Risk analytics dashboard","‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§°"))

    if st.session_state.df is None or st.session_state.pred_df is None:
        st.info(T("Please upload and train a model first.","‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡§π‡§≤‡•á ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç‡•§"))
    else:
        df = st.session_state.df
        pred_df = st.session_state.pred_df
        risk_labels = st.session_state.risk_labels
        detected = st.session_state.detected
        feature_names = (st.session_state.train_result or {}).get("feature_names", [])
        alerts_df = st.session_state.alerts_df

        if alerts_df is not None and len(alerts_df) > 0:
            st.markdown('<div class="alert-banner">', unsafe_allow_html=True)
            st.markdown(f"**{T('Alerts','‡§Ö‡§≤‡§∞‡•ç‡§ü')}:** {len(alerts_df)}")
            st.markdown('</div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="success-banner">', unsafe_allow_html=True)
            st.markdown("**" + T("Status: No critical alerts","‡§∏‡•ç‡§•‡§ø‡§§‡§ø: ‡§ï‡•ã‡§à ‡§ó‡§Ç‡§≠‡•Ä‡§∞ ‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç") + "**")
            st.markdown('</div>', unsafe_allow_html=True)

        c1, c2 = st.columns(2)
        with c1:
            st.markdown("#### " + T("Risk probability distribution","‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ ‡§µ‡§ø‡§§‡§∞‡§£"))
            st.plotly_chart(make_risk_distribution_plot(pred_df["probability"].values), use_container_width=True)
        with c2:
            st.markdown("#### " + T("Composition of risk levels","‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡•ç‡§§‡§∞‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ"))
            st.plotly_chart(make_risk_pie(risk_labels), use_container_width=True)

        if st.session_state.train_result is not None and len(feature_names) > 0:
            st.markdown("#### " + T("Feature importance (SHAP)","‡§´‡•Ä‡§ö‡§∞ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ (SHAP)"))
            try:
                train_res = st.session_state.train_result
                imp_df = explain_shap(train_res["model"], train_res["X_train_proc"], feature_names)["importance_df"]
                st.plotly_chart(make_importance_bar(imp_df, top_n=20), use_container_width=True)
            except Exception:
                st.info(T("Feature importance not available.","‡§´‡•Ä‡§ö‡§∞ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç‡•§"))

        st.markdown("#### " + T("Top high-risk students","‡§∂‡•Ä‡§∞‡•ç‡§∑ ‡§â‡§ö‡•ç‡§ö-‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§õ‡§æ‡§§‡•ç‡§∞"))
        merged = df.copy()
        merged["dropout_probability"] = pred_df["probability"].values
        merged["risk_level"] = risk_labels
        merged_sorted = merged.sort_values("dropout_probability", ascending=False)
        top_n = st.slider(T("How many to show?","‡§ï‡§ø‡§§‡§®‡•á ‡§¶‡§ø‡§ñ‡§æ‡§è‡§Å?"), 5, 100, 20, 5)
        st.dataframe(merged_sorted.head(top_n), use_container_width=True)

        st.markdown("#### " + T("Alerts panel","‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§™‡•à‡§®‡§≤"))
        if alerts_df is not None and len(alerts_df) > 0:
            st.dataframe(alerts_df, use_container_width=True)
        else:
            st.write(T("No alerts.","‡§ï‡•ã‡§à ‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç‡•§"))
    st.markdown('</div>', unsafe_allow_html=True)

# -----------------------------
# Student Search
# -----------------------------
elif page == T("Student Search","‡§õ‡§æ‡§§‡•ç‡§∞ ‡§ñ‡•ã‡§ú"):
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.markdown("### üîé " + T("Student search & instant explanations","‡§õ‡§æ‡§§‡•ç‡§∞ ‡§ñ‡•ã‡§ú ‡§î‡§∞ ‡§§‡§æ‡§§‡•ç‡§ï‡•ç‡§∑‡§£‡§ø‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ"))

    if st.session_state.df is None or st.session_state.pred_df is None or st.session_state.train_result is None:
        st.info(T("Please upload and train a model first.","‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡§π‡§≤‡•á ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç‡•§"))
    else:
        df = st.session_state.df
        pred_df = st.session_state.pred_df
        risk_labels = st.session_state.risk_labels
        detected = st.session_state.detected
        id_col = detected["id_col"] if detected else None
        feature_cols = (st.session_state.train_result["X_train"].columns.tolist()
                        if st.session_state.train_result is not None else [])
        feature_names = st.session_state.train_result["feature_names"]

        query = st.text_input(T("Search by Student ID, Name, Roll No, or Email","‡§õ‡§æ‡§§‡•ç‡§∞ ‡§Ü‡§à‡§°‡•Ä, ‡§®‡§æ‡§Æ, ‡§∞‡•ã‡§≤ ‡§®‡§Ç‡§¨‡§∞ ‡§Ø‡§æ ‡§à‡§Æ‡•á‡§≤ ‡§∏‡•á ‡§ñ‡•ã‡§ú‡•á‡§Ç"))
        if query:
            qnorm = query.strip().lower()
            candidates = []
            id_candidates = [id_col] if id_col else []
            id_candidates += [c for c in df.columns if any(k in _normalized(c) for k in ["id","roll","email","name","adm","reg"])]
            id_candidates = list(dict.fromkeys([c for c in id_candidates if c in df.columns]))

            for c in id_candidates:
                try:
                    mask = df[c].astype(str).str.lower().str.contains(qnorm, na=False)
                    idxs = df.index[mask].tolist()
                    for ix in idxs:
                        candidates.append(ix)
                except Exception:
                    continue
            candidates = list(dict.fromkeys(candidates))

            if len(candidates) == 0:
                st.warning(T("No matching students found.","‡§ï‡•ã‡§à ‡§Æ‡§ø‡§≤‡§§‡•á-‡§ú‡•Å‡§≤‡§§‡•á ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á‡•§"))
            else:
                st.success(T(f"Found {len(candidates)} record(s).", f"{len(candidates)} ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§Æ‡§ø‡§≤‡•á‡•§"))
                sel_ix = st.selectbox(T("Select a record index","‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§á‡§Ç‡§°‡•á‡§ï‡•ç‡§∏ ‡§ö‡•Å‡§®‡•á‡§Ç"), options=candidates, index=0)

                prob = pred_df.loc[sel_ix, "probability"]
                label = risk_label(prob, thresholds=(low_th, high_th))
                badge_html = style_risk_cell(label)
                st.markdown(f"**{T('Risk level','‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∏‡•ç‡§§‡§∞')}:** {badge_html} ‚Äî **{T('Probability','‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ')}:** {prob:.3f}", unsafe_allow_html=True)

                att_summary = _attendance_summary(df, detected["attendance_cols"])
                mk_summary = _marks_summary(df, detected["marks_cols"])
                if att_summary or mk_summary:
                    st.markdown("#### " + T("Summary","‡§∏‡§æ‡§∞"))
                    if att_summary: st.write(f"**{T('Label:','‡§≤‡•á‡§¨‡§≤:')}** {att_summary}")
                    if mk_summary: st.write(f"**{T('Label:','‡§≤‡•á‡§¨‡§≤:')}** {mk_summary}")

                st.markdown("#### " + T("SHAP per-student explanation","SHAP ‡§™‡•ç‡§∞‡§§‡§ø-‡§õ‡§æ‡§§‡•ç‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ"))
                try:
                    train_res = st.session_state.train_result
                    X_row = df.loc[[sel_ix], feature_cols]
                    X_row_proc = train_res["preprocessor"].transform(X_row)
                    shap_res = explain_shap(train_res["model"], X_row_proc, feature_names, index=0)

                    # Waterfall (matplotlib)
                    st.write("**" + T("Contribution to dropout probability","‡§°‡•ç‡§∞‡•â‡§™‡§Ü‡§â‡§ü ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§®") + "**")
                    plt.figure(figsize=(8,6))
                    shap.plots._waterfall.waterfall_legacy(
                        shap_res["base_value"],
                        shap_res["sample_shap"],
                        feature_names=feature_names,
                        max_display=12,
                        show=False
                    )
                    st.pyplot(plt.gcf(), use_container_width=True)
                    plt.close()

                    # Top contributors bar (plotly)
                    order = np.argsort(-np.abs(shap_res["sample_shap"]))
                    top_df = pd.DataFrame({"feature": np.array(feature_names)[order][:12],
                                           "shap_value": shap_res["sample_shap"][order][:12]})
                    fig_bar = go.Figure(go.Bar(
                        x=top_df["shap_value"],
                        y=top_df["feature"],
                        orientation="h",
                        marker_color=["#D8345F" if v > 0 else "#4FB37A" for v in top_df["shap_value"]]
                    ))
                    fig_bar.update_layout(
                        title=T("Top feature contributions (positive = increase risk)","‡§∂‡•Ä‡§∞‡•ç‡§∑ ‡§´‡•Ä‡§ö‡§∞ ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® (‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï = ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§¨‡§¢‡§º‡§§‡§æ ‡§π‡•à)"),
                        xaxis_title="SHAP",
                        yaxis_title=T("Feature","‡§´‡•Ä‡§ö‡§∞"),
                        height=500,
                        margin=dict(l=0, r=0, t=30, b=0),
                    )
                    st.plotly_chart(fig_bar, use_container_width=True)

                    st.markdown("#### " + T("Counseling suggestions","‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§∏‡•Å‡§ù‡§æ‡§µ"))
                    for s in generate_counseling(label):
                        st.write(f"- {s}")

                except Exception as e:
                    st.error(T(f"SHAP explanation unavailable: {e}", f"SHAP ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç: {e}"))

                student_row = df.loc[[sel_ix]].copy()
                student_row["dropout_probability"] = prob
                student_row["risk_level"] = label
                st.download_button(T("Download this student's report (CSV)","‡§á‡§∏ ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§ï‡•Ä ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç (CSV)"),
                                   data=student_row.to_csv(index=False).encode("utf-8"),
                                   file_name=f"student_{sel_ix}_report.csv", mime="text/csv")

    st.markdown('</div>', unsafe_allow_html=True)

# -----------------------------
# Explainability (SHAP)
# -----------------------------
elif page == T("Explainability (SHAP)","‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡•á‡§Ø‡§§‡§æ (SHAP)"):
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.markdown("### üß† " + T("Global explainability with SHAP","SHAP ‡§ï‡•á ‡§∏‡§æ‡§• ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡•á‡§Ø‡§§‡§æ"))

    if st.session_state.train_result is None:
        st.info(T("Train a model first.","‡§™‡§π‡§≤‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç‡•§"))
    else:
        train_res = st.session_state.train_result
        feature_names = train_res["feature_names"]
        X_train_proc = train_res["X_train_proc"]
        model = train_res["model"]

        try:
            shap_res = explain_shap(model, X_train_proc, feature_names)
            imp_df = shap_res["importance_df"]

            st.write("**" + T("SHAP summary (beeswarm)","SHAP ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ (‡§¨‡•Ä‡§∏‡•ç‡§µ‡•â‡§∞‡•ç‡§Æ)") + "**")
            plt.figure(figsize=(9,6))
            shap.summary_plot(shap_res["shap_values"], X_train_proc, feature_names=feature_names, show=False)
            st.pyplot(plt.gcf(), use_container_width=True)
            plt.close()

            st.write("**" + T("SHAP global bar chart","SHAP ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§¨‡§æ‡§∞ ‡§ö‡§æ‡§∞‡•ç‡§ü") + "**")
            st.plotly_chart(make_importance_bar(imp_df, top_n=25), use_container_width=True)
        except Exception as e:
            st.error(T(f"Unable to render SHAP plots: {e}", f"SHAP ‡§™‡•ç‡§≤‡•â‡§ü ‡§∞‡•á‡§Ç‡§°‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡•á: {e}"))
    st.markdown('</div>', unsafe_allow_html=True)

# -----------------------------
# Counseling & Mentoring
# -----------------------------
elif page == T("Counseling & Mentoring","‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§î‡§∞ ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"):
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.markdown("### ü§ù " + T("Counseling & peer mentoring","‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§î‡§∞ ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"))

    if st.session_state.df is None or st.session_state.pred_df is None:
        st.info(T("Train a model to generate counseling and mentoring.","‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§î‡§∞ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡•á‡§Ç‡•§"))
    else:
        df = st.session_state.df
        pred_df = st.session_state.pred_df
        risk_labels = st.session_state.risk_labels
        detected = st.session_state.detected
        id_col = detected["id_col"]

        merged = df.copy()
        merged["dropout_probability"] = pred_df["probability"].values
        merged["risk_level"] = risk_labels

        high_df = merged[merged["risk_level"] == "High"].sort_values("dropout_probability", ascending=False)
        med_df = merged[merged["risk_level"] == "Medium"].sort_values("dropout_probability", ascending=False)

        st.markdown("#### " + T("High-risk ‚Äì mandatory peer mentoring","‡§â‡§ö‡•ç‡§ö ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‚Äì ‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"))
        if len(high_df) == 0:
            st.write(T("No high-risk students at the moment.","‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ï‡•ã‡§à ‡§â‡§ö‡•ç‡§ö-‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§®‡§π‡•Ä‡§Ç‡•§"))
        else:
            show_cols = [id_col] + [c for c in merged.columns if c != id_col] if id_col in merged.columns else merged.columns
            st.dataframe(high_df[show_cols].head(50), use_container_width=True)

        st.markdown("#### " + T("Medium-risk ‚Äì suggested group mentoring","‡§Æ‡§ß‡•ç‡§Ø‡§Æ ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‚Äì ‡§∏‡§Æ‡•Ç‡§π ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®"))
        if len(med_df) == 0:
            st.write(T("No medium-risk students at the moment.","‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ï‡•ã‡§à ‡§Æ‡§ß‡•ç‡§Ø‡§Æ-‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§®‡§π‡•Ä‡§Ç‡•§"))
        else:
            show_cols = [id_col] + [c for c in merged.columns if c != id_col] if id_col in merged.columns else merged.columns
            st.dataframe(med_df[show_cols].head(50), use_container_width=True)

        st.markdown("#### " + T("Mentor assignment logic","‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï ‡§Ö‡§∏‡§æ‡§á‡§®‡§Æ‡•á‡§Ç‡§ü ‡§≤‡•â‡§ú‡§ø‡§ï"))
        st.write(T(
            "We suggest mentors who have good attendance and marks, and pair them with students from similar classes.",
            "‡§π‡§Æ ‡§â‡§® ‡§∏‡§æ‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï ‡§∏‡•Å‡§ù‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡§ø‡§®‡§ï‡•Ä ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§î‡§∞ ‡§Ö‡§Ç‡§ï ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§π‡•à‡§Ç, ‡§î‡§∞ ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§∏‡§Æ‡§æ‡§® ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§ï‡•á ‡§õ‡§æ‡§§‡•ç‡§∞‡•ã‡§Ç ‡§∏‡•á ‡§ú‡•ã‡§°‡§º‡§§‡•á ‡§π‡•à‡§Ç‡•§"
        ))

        att_cols = detected["attendance_cols"]
        mk_cols = detected["marks_cols"]
        potential_mentors = merged.copy()
        try:
            att_val = potential_mentors[att_cols].applymap(safe_float).mean(axis=1) if att_cols else 0
            mk_val = potential_mentors[mk_cols].applymap(safe_float).mean(axis=1) if mk_cols else 0
            potential_mentors["mentor_score"] = 0.5 * (att_val if hasattr(att_val, "fillna") else att_val) + \
                                                0.5 * (mk_val if hasattr(mk_val, "fillna") else mk_val)
        except Exception:
            potential_mentors["mentor_score"] = 0.5

        mentor_df = potential_mentors[potential_mentors["risk_level"] == "Low"].sort_values("mentor_score", ascending=False).head(50)
        id_like_cols = [c for c in merged.columns if any(k in _normalized(c) for k in ["id","name","email","roll"])]
        cols_show = list(dict.fromkeys(id_like_cols + ["risk_level","dropout_probability","mentor_score"]))
        st.markdown("#### " + T("Potential peer mentors","‡§∏‡§Ç‡§≠‡§æ‡§µ‡§ø‡§§ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï"))
        st.dataframe(mentor_df[cols_show] if len(cols_show) > 0 else mentor_df.head(20), use_container_width=True)

        st.markdown("#### " + T("Auto-match mentors to high-risk students (demo)","‡§â‡§ö‡•ç‡§ö-‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§õ‡§æ‡§§‡•ç‡§∞‡•ã‡§Ç ‡§∏‡•á ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï ‡§ï‡§æ ‡§ë‡§ü‡•ã-‡§Æ‡•à‡§ö (‡§°‡•á‡§Æ‡•ã)"))
        try:
            assign_count = min(len(high_df), len(mentor_df), 20)
            matches = []
            for i in range(assign_count):
                mentee_row = high_df.iloc[i]
                mentor_row = mentor_df.iloc[i]
                mentee_id = mentee_row[id_col] if id_col in high_df.columns else f"Index {mentee_row.name}"
                mentor_id = mentor_row[id_col] if id_col in mentor_df.columns else f"Index {mentor_row.name}"
                matches.append({T("Mentee","‡§Æ‡•á‡§Ç‡§ü‡•á"): mentee_id,
                                T("Mentor","‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï"): mentor_id,
                                T("Mentor score","‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï ‡§∏‡•ç‡§ï‡•ã‡§∞"): mentor_row.get("mentor_score", np.nan)})
            if len(matches) > 0:
                st.dataframe(pd.DataFrame(matches), use_container_width=True)
            else:
                st.write(T("Not enough mentors to match.","‡§Æ‡•à‡§ö ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§ï ‡§®‡§π‡•Ä‡§Ç‡•§"))
        except Exception:
            st.write(T("Mentor matching unavailable for this dataset; please ensure ID columns are present.",
                       "‡§á‡§∏ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•à‡§ö‡§ø‡§Ç‡§ó ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç; ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ü‡§à‡§°‡•Ä ‡§ï‡•â‡§≤‡§Æ ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç‡•§"))
    st.markdown('</div>', unsafe_allow_html=True)

# -----------------------------
# Footer
# -----------------------------
st.markdown('<div class="glass-card">', unsafe_allow_html=True)
st.markdown("### üìù " + T("Notes","‡§®‡•ã‡§ü‡•ç‡§∏"))
st.write(T(
    "This app uses a robust pipeline and XGBoost for accuracy. SHAP explains which features impact risk. Alerts trigger for high risk, repeated medium risk, and low attendance.",
    "‡§Ø‡§π ‡§ê‡§™ ‡§∏‡§ü‡•Ä‡§ï‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§™‡§æ‡§á‡§™‡§≤‡§æ‡§á‡§® ‡§î‡§∞ XGBoost ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ SHAP ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•å‡§® ‡§∏‡•á ‡§´‡•Ä‡§ö‡§∞ ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§ï‡•ã ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§Ö‡§≤‡§∞‡•ç‡§ü ‡§â‡§ö‡•ç‡§ö ‡§ú‡•ã‡§ñ‡§ø‡§Æ, ‡§¨‡§æ‡§∞-‡§¨‡§æ‡§∞ ‡§Æ‡§ß‡•ç‡§Ø‡§Æ ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§î‡§∞ ‡§ï‡§Æ ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§™‡§∞ ‡§ü‡•ç‡§∞‡§ø‡§ó‡§∞ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§"
))
st.markdown('</div>', unsafe_allow_html=True)
